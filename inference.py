import json
from utils.model import BertClassification, TextEmbedder
from transformers import BertTokenizer
from utils.config import device
import torch
import socket
import threading

socket_host = '127.0.0.1'
send_socket_port = 8125
listen_socket_port = 8124
listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
listen_socket.bind((socket_host, listen_socket_port))
listen_socket.listen(5)

sen_model = BertClassification.from_pretrained('./bert-label-classification').to(device)
sen_tokenizer = BertTokenizer.from_pretrained('./bert-label-classification')
llm_binary_model = TextEmbedder(num_classes=2).to(device)
llm_binary_model.load_state_dict(torch.load('./checkpoints_llm/checkpoint_epoch_2.pth', map_location=device))
llm_classifier_model = TextEmbedder(num_classes=14).to(device)
llm_classifier_model.load_state_dict(torch.load('./checkpoints_llm/checkpoint_epoch_1_14.pth', map_location=device))
llm_tokenizer = BertTokenizer.from_pretrained('./bert-large-uncased')
sen_model.eval()
llm_binary_model.eval()
llm_classifier_model.eval()

llm_labels = ["kimi", "通义", "文心一言", "智谱", "ChatGLM3-6B", "QWen1.5-7B", "QWen1.5-14B", "Baichuan2-7B",
              "Baichuan2-13B", "ChatGPT", "Llama2-7B", "Llama2-13B", "Llama3-8B", "Mistral v0.2 7B"]


def predict_sensitive(content: str, max_length: int = 512) -> (bool, float):
    input_ids = sen_tokenizer(content, max_length=max_length, truncation=True, return_tensors='pt')['input_ids'].to(
        device)
    with torch.no_grad():
        logits = sen_model(input_ids)[0]
        score = torch.softmax(logits, dim=0)[0].item()
        pred = logits.argmax().item()

    return True if pred == 0 else False, round(score, 5)


def predict_llm(content: str, max_length: int = 512) -> dict:
    input_ids = llm_tokenizer(content, max_length=max_length, truncation=True, return_tensors='pt')['input_ids'].to(
        device)
    with torch.no_grad():
        logits = llm_binary_model(input_ids)
        probs = torch.softmax(logits / 10, dim=1)
        generated_prob = probs[0][1].item()

    if generated_prob > 0.5:
        with torch.no_grad():
            logits = llm_classifier_model(input_ids)
            weights = torch.tensor(
                [0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.4, 0.046, 0.046, 0.046, 0.046],
                device=device)
            weighted_logits = logits * weights
            probs = torch.softmax(weighted_logits, dim=1)[0]

        llm_probabilities = dict(zip(llm_labels, probs.tolist()))

        return {
            'llm_probability': generated_prob,
            'llm_class_probability': llm_probabilities
        }
    else:
        return {
            'llm_probability': generated_prob,
            'message': 'Not Generated by LLM'
        }


def process_daily_data(id, content):
    is_sensitive, score = predict_sensitive(content)
    llm_result = predict_llm(content)
    is_bot_score = llm_result['llm_probability']
    is_bot = True if is_bot_score > 0.5 else False

    results = [id, is_sensitive, score, is_bot, is_bot_score]
    if is_bot:
        results.append(max(llm_result['llm_class_probability'],
                           key=llm_result['llm_class_probability'].get))

    try:
        send_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        send_socket.connect((socket_host, send_socket_port))
        send_socket.sendall(json.dumps(results).encode())
        send_socket.close()
    except:
        return


def socket_program():
    print('Listening...')
    while True:
        conn, address = listen_socket.accept()
        print(f"Connection from {address} has been established.")
        data = b""
        while True:
            packet = conn.recv(4096)
            if not packet:
                break
            data += packet
        conn.close()
        if len(data):
            id, content = json.loads(data.decode())
            if content:
                print(id, content)
                process_daily_data(id, content)


if __name__ == '__main__':
    socket_program()
